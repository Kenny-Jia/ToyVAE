{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406db2a-9fbf-450e-9968-a31ec6d51c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfmath\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ecd26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import PReLU, Input, LSTM, Flatten, Concatenate, Dense, Conv2D, TimeDistributed, MaxPooling2D, LeakyReLU, ReLU, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from qkeras import QActivation, QDense, QConv2D, QBatchNormalization, QConv2DBatchnorm\n",
    "from qkeras import quantized_relu, quantized_bits\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc21ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed_SNL_data.h5 is located under s3df /sdf/home/h/hjia625/\n",
    "with h5py.File('/sdf/home/h/hjia625/preprocessed_SNL_data.h5', 'r') as hf:\n",
    "    X_train = hf['X_train'][:]\n",
    "    X_test  = hf['X_test'][:]\n",
    "    Ato4l_data  = hf['Ato4l_data'][:]\n",
    "    hToTauTau_data  = hf['hToTauTau_data'][:]\n",
    "    hChToTauNu_data  = hf['hChToTauNu_data'][:]\n",
    "    leptoquark_data = hf['leptoquark_data'][:]\n",
    "    print(\"Data loaded from preprocessed_SNL_data.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"/eos/user/h/hjia/AnomalyDetection/liam_AD/training_notebooks/SM_ZeroBias_dataset.h5\"\n",
    "# with h5py.File(filename, 'r') as file:\n",
    "#     X_train = np.array(file['X_train'])\n",
    "#     X_test = np.array(file['X_test'])\n",
    "#     X_val = np.array(file['X_val'])\n",
    "# X_train = np.vstack((X_train, X_val))\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81033d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_columns(arr):\n",
    "#     # Check if the input array has 57 columns\n",
    "#     if arr.shape[1] != 57:\n",
    "#         raise ValueError(\"Input array must have 57 columns\")\n",
    "    \n",
    "#     # Create a copy of the input array to avoid modifying the original\n",
    "#     scaled_arr = arr.copy()\n",
    "# #     met_scale = 1\n",
    "# #     em_scale = 1\n",
    "# #     tau_scale = 1\n",
    "# #     jet_scale = 1\n",
    "# #     muon_scale = 1\n",
    "#     met_scale = 512\n",
    "#     em_scale = 128\n",
    "#     tau_scale = 128\n",
    "#     jet_scale = 256\n",
    "#     muon_scale = 32\n",
    "#     # Define the scaling factors for each column\n",
    "#     scale_dict = {\n",
    "#         0: 1/met_scale,\n",
    "#         3: 1/em_scale, 6: 1/em_scale, 9: 1/em_scale, 12: 1/em_scale, 15: 1/tau_scale,\n",
    "#         18: 1/tau_scale, 21: 1/tau_scale, 24: 1/tau_scale,\n",
    "#         27: 1/jet_scale, 30: 1/jet_scale, 33: 1/jet_scale, 36: 1/jet_scale, 39: 1/jet_scale, 42: 1/jet_scale,\n",
    "#         45: 1/muon_scale, 48: 1/muon_scale, 51: 1/muon_scale, 54: 1/muon_scale\n",
    "#     }\n",
    "\n",
    "#     # Apply scaling to the specified columns\n",
    "#     for col, scale_factor in scale_dict.items():\n",
    "#         scaled_arr[:, col] *= scale_factor\n",
    "    \n",
    "#     return scaled_arr\n",
    "# X_train = scale_columns(X_train)\n",
    "# X_test = scale_columns(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b95c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eedb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_columns(arr):\n",
    "    # Check if the input array has 57 columns\n",
    "    if arr.shape[1] != 57:\n",
    "        raise ValueError(\"Input array must have 57 columns\")\n",
    "\n",
    "    for col in range(57):\n",
    "        column = arr[:, col]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_val = np.mean(column)\n",
    "        min_val = np.min(column)\n",
    "        max_val = np.max(column)\n",
    "        has_nan = np.isnan(column).any()\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Column {col + 1}:\")\n",
    "        print(f\"  Mean: {mean_val:.4f}\")\n",
    "        print(f\"  Min: {min_val:.4f}\")\n",
    "        print(f\"  Max: {max_val:.4f}\")\n",
    "        print(f\"  Contains NaN: {has_nan}\")\n",
    "        print()  # Empty line for readability\n",
    "analyze_columns(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd108b6-f2b4-4b39-9cee-ba8faa7bc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def Qmake_encoder_set_weights(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    l2_factor = 1e-3\n",
    "    inputs = keras.Input(shape=(input_dim))\n",
    "#     x = BatchNormalization(name=\"BN0\")(inputs)\n",
    "    x = Dense(h_dim_1,\n",
    "             kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "             bias_initializer=keras.initializers.Zeros(),\n",
    "             kernel_regularizer=l1_l2(l1=0, l2=l2_factor),\n",
    "             name = \"dense1\")(inputs)\n",
    "#     x = BatchNormalization(name=\"BN1\")(x)\n",
    "#     x = ReLU(name=\"relu1\")(x)\n",
    "    x = ReLU(name=\"relu1\")(x)\n",
    "    x = Dense(h_dim_2,\n",
    "             kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "             bias_initializer=keras.initializers.Zeros(),\n",
    "             kernel_regularizer=l1_l2(l1=0, l2=l2_factor),\n",
    "             name = \"dense2\")(x)    \n",
    "#     x = BatchNormalization(name=\"BN2\")(x)\n",
    "    x = ReLU(name=\"relu2\")(x)\n",
    "    z_mean=Dense(latent_dim, name='z_mean',\n",
    "                  kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                  bias_initializer=keras.initializers.Zeros(),\n",
    "                  kernel_regularizer=l1_l2(l1=0, l2=l2_factor))(x)\n",
    "    z_logvar=Dense(latent_dim, name='z_log_var',\n",
    "                          kernel_initializer=keras.initializers.Zeros(),\n",
    "                          bias_initializer=keras.initializers.Zeros(),\n",
    "                          kernel_regularizer=l1_l2(l1=0, l2=l2_factor))(x)\n",
    "    z=Sampling()([z_mean,z_logvar])\n",
    "    encoder = keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "    return encoder\n",
    "\n",
    "def Qmake_decoder_set_weights(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    l2_factor = 1e-3\n",
    "    inputs=keras.Input(shape=(latent_dim))\n",
    "    x=layers.Dense(h_dim_2,\n",
    "                   activation='relu',\n",
    "                   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                   bias_initializer=keras.initializers.Zeros(),\n",
    "                   kernel_regularizer=l1_l2(l1=0, l2=l2_factor))(inputs)\n",
    "    x=layers.Dense(h_dim_1,\n",
    "                   activation='relu',\n",
    "                   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                   bias_initializer=keras.initializers.Zeros(),\n",
    "                   kernel_regularizer=l1_l2(l1=0, l2=l2_factor))(x)\n",
    "    z=layers.Dense(input_dim,\n",
    "                   kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                   bias_initializer=keras.initializers.Zeros(),\n",
    "                   kernel_regularizer=l1_l2(l1=0, l2=l2_factor))(x)\n",
    "    decoder=keras.Model(inputs,z,name='decoder')\n",
    "    return decoder\n",
    "\n",
    "def custom_mse_loss_with_multi_index_scaling(masked_data, masked_reconstruction):\n",
    "#     jet_scale = 256/64\n",
    "#     tau_scale = 128/64\n",
    "#     muon_scale = 32/64\n",
    "#     met_scale = 512/64\n",
    "#     em_scale = 128/64\n",
    "    jet_scale = 1\n",
    "    tau_scale = 1\n",
    "    muon_scale = 1\n",
    "    met_scale = 1\n",
    "    em_scale = 1\n",
    "    # Define the indices and their corresponding scale factors\n",
    "    scale_dict = {\n",
    "        0: met_scale,\n",
    "        3: em_scale, 6: em_scale, 9: em_scale, 12: em_scale,\n",
    "        15: tau_scale, 18: tau_scale, 21: tau_scale, 24: tau_scale,\n",
    "        27: jet_scale, 30: jet_scale, 33: jet_scale, 36: jet_scale, 39: jet_scale, 42: jet_scale,\n",
    "        45: muon_scale, 48: muon_scale, 51: muon_scale, 54: muon_scale\n",
    "    }\n",
    "\n",
    "    # Create the scaling tensor\n",
    "    scale_tensor = tf.ones_like(masked_data)\n",
    "    for index, factor in scale_dict.items():\n",
    "        index_mask = tf.one_hot(index, depth=tf.shape(masked_data)[-1])\n",
    "        scale_tensor += index_mask * (factor - 1)\n",
    "\n",
    "    # Apply scaling\n",
    "    scaled_data = masked_data * scale_tensor\n",
    "    scaled_reconstruction = masked_reconstruction * scale_tensor\n",
    "\n",
    "    # Hardcoded lists for eta and phi indices\n",
    "    eta_indices = [4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52, 55]\n",
    "    phi_indices = [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47, 50, 53, 56]\n",
    "\n",
    "    batch_size = tf.shape(scaled_reconstruction)[0]\n",
    "    \n",
    "    # Set only the first eta (index 1) to zero\n",
    "    indices = tf.stack([tf.range(batch_size), tf.ones(batch_size, dtype=tf.int32)], axis=1)\n",
    "    updates = tf.zeros(batch_size)\n",
    "    scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    \n",
    "    # Apply constraints to eta\n",
    "    for i in eta_indices:\n",
    "        indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "        updates = 3 * tf.tanh(scaled_reconstruction[:, i] / 3)\n",
    "        scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "    \n",
    "    # Apply constraints to phi\n",
    "    for i in phi_indices:\n",
    "        indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], i)], axis=1)\n",
    "        updates = 3.14159265258979 * tf.tanh(scaled_reconstruction[:, i] / 3.14159265258979)\n",
    "        scaled_reconstruction = tf.tensor_scatter_nd_update(scaled_reconstruction, indices, updates)\n",
    "        \n",
    "    # Calculate MSE using keras.losses.mse\n",
    "    mse = keras.losses.mse(scaled_data, scaled_reconstruction)\n",
    "\n",
    "    # Take the sum across all dimensions\n",
    "    return tf.reduce_sum(mse)\n",
    "\n",
    "class VAE_Model(keras.Model):\n",
    "    def __init__(self, encoder, decoder, steps_per_epoch=3125, cycle_length=10, min_beta=0.1, max_beta=0.85, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.beta_tracker = keras.metrics.Mean(name=\"beta\")\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.cycle_length = tf.cast(cycle_length, tf.float32)\n",
    "        self.min_beta = tf.cast(min_beta, tf.float32)\n",
    "        self.max_beta = tf.cast(max_beta, tf.float32)\n",
    "        self.beta = tf.Variable(min_beta, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.beta_tracker,\n",
    "        ]\n",
    "\n",
    "    def cyclical_annealing_beta(self, epoch):\n",
    "        cycle = tf.floor(1.0 + epoch / self.cycle_length)\n",
    "        x = tf.abs(epoch / self.cycle_length - cycle + 1)\n",
    "        return self.min_beta + (self.max_beta - self.min_beta) * tf.minimum(x, 1.0)\n",
    "#     def set_beta(self,beta):\n",
    "#         self.beta=beta\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Get the current epoch number\n",
    "        epoch = tf.cast(self.optimizer.iterations / self.steps_per_epoch, tf.float32)\n",
    "        \n",
    "        # Update beta\n",
    "        self.beta.assign(self.cyclical_annealing_beta(epoch))\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "#             reconstruction_loss = tf.reduce_mean(keras.losses.mse(mask*data, mask*reconstruction))\n",
    "            reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*data, mask*reconstruction)\n",
    "            reconstruction_loss *=(1-self.beta)\n",
    "\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *=self.beta\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reco_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"beta\": self.beta,\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "        reconstruction_loss = custom_mse_loss_with_multi_index_scaling(mask*data, mask*reconstruction)\n",
    "#         reconstruction_loss = tf.reduce_mean(keras.losses.mse(mask*data, mask*reconstruction))\n",
    "        reconstruction_loss*=(1-self.beta)\n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(kl_loss)        \n",
    "        kl_loss *=self.beta\n",
    "        \n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reco_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "            \"beta\": self.beta,\n",
    "        }\n",
    "\n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse(y_true, y_pred, sample_weight):\n",
    "    return tf.reduce_mean(tf.multiply(sample_weight, tf.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "STOP_PATIENCE = 15\n",
    "LR_PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b66b6-96f3-4e7d-ac72-d24cd1002dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TOPO 2A Trainer\n",
    "\n",
    "T2A_enc = Qmake_encoder_set_weights(X_train.shape[1],128,64,16)\n",
    "T2A_dec = Qmake_decoder_set_weights(X_train.shape[1],128,64,16)\n",
    "steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "T2A = VAE_Model(T2A_enc, T2A_dec, steps_per_epoch=steps_per_epoch, cycle_length=10, min_beta=0.1, max_beta=0.8)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "T2A.compile(optimizer=opt,weighted_metrics=[weighted_mse]) #,weighted_metrics=[weighted_mse]\n",
    "# T2A.build(input_shape=(997315, 110))\n",
    "T2A_enc.summary()\n",
    "T2A_dec.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbfc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "history = T2A.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, shuffle=True)\n",
    "T2A.save_weights(filepath='/fs/ddn/sdf/group/atlas/d/hjia625/ToyVAE/software_dev/software_model/checkpoint_large/', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'history' is the object returned by your model.fit() call\n",
    "\n",
    "# Extract the loss values\n",
    "total_loss = history.history['loss']\n",
    "reco_loss = history.history['reco_loss']\n",
    "kl_loss = history.history['kl_loss']\n",
    "val_total_loss = history.history['val_loss']\n",
    "val_reco_loss = history.history['val_reco_loss']\n",
    "val_kl_loss = history.history['val_kl_loss']\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot training losses\n",
    "plt.plot(total_loss, label='Total Loss', color='blue')\n",
    "plt.plot(reco_loss, label='Reconstruction Loss', color='green')\n",
    "plt.plot(kl_loss, label='KL Loss', color='red')\n",
    "\n",
    "# Plot validation losses\n",
    "plt.plot(val_total_loss, label='Val Total Loss', color='blue', linestyle='--')\n",
    "plt.plot(val_reco_loss, label='Val Reconstruction Loss', color='green', linestyle='--')\n",
    "plt.plot(val_kl_loss, label='Val KL Loss', color='red', linestyle='--')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat testing\n",
    "for i in range(0,20):\n",
    "    T2A_enc = Qmake_encoder_set_weights(X_train.shape[1],128,64,16)\n",
    "    T2A_dec = Qmake_decoder_set_weights(X_train.shape[1],128,64,16)\n",
    "    steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "    T2A = VAE_Model(T2A_enc, T2A_dec, steps_per_epoch=steps_per_epoch, cycle_length=10, min_beta=0.1, max_beta=0.7)\n",
    "    # T2A.set_beta(beta)\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    T2A.compile(optimizer=opt,weighted_metrics=[weighted_mse]) #,weighted_metrics=[weighted_mse]\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "    callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    history = T2A.fit(x=X_train, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, shuffle=True)\n",
    "    T2A.save_weights(filepath='/fs/ddn/sdf/group/atlas/d/hjia625/ToyVAE/software_dev/software_model/checkpoint_large/Repeat_Testing/large_{}/'.format(i), save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals_file = [\"/eos/user/h/hjia/AnomalyDetection/liam_AD/training_notebooks/Ato4l_dataset.h5\",\n",
    "#                 \"/eos/user/h/hjia/AnomalyDetection/liam_AD/training_notebooks/hToTauTau_dataset.h5\",\n",
    "#                 \"/eos/user/h/hjia/AnomalyDetection/liam_AD/training_notebooks/hChToTauNu_dataset.h5\",\n",
    "#                 \"/eos/user/h/hjia/AnomalyDetection/liam_AD/training_notebooks/leptoquark_dataset.h5\"]\n",
    "signal_labels = [\"Ato4l\",\n",
    "                \"hToTauTau\",\n",
    "                \"hChToTauNu\",\n",
    "                \"leptoquark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = [Ato4l_data,\n",
    "               hToTauTau_data,\n",
    "               hChToTauNu_data,\n",
    "               leptoquark_data]\n",
    "# for i, label in enumerate(signal_labels):\n",
    "#     with h5py.File(signals_file[i], 'r') as file:\n",
    "#         test_data = np.array(scaler.transform(file['Data']))\n",
    "# #         test_data = scale_columns(np.array(file['Data']))\n",
    "#         signal_data.append(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AD_score_KL(z_mean, z_log_var):\n",
    "    kl_loss = np.mean(-0.5 * (1 + z_log_var - (z_mean) ** 2 - np.exp(z_log_var)))\n",
    "    return kl_loss\n",
    "\n",
    "def AD_score_CKL(z_mean, z_log_var):\n",
    "    CKL = np.mean(z_mean**2)\n",
    "    return CKL\n",
    "\n",
    "class Model_Evaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        background,\n",
    "        br_weights,\n",
    "        signal,\n",
    "        signal_weights,\n",
    "        input_dim,\n",
    "        title=\"placeholder\",\n",
    "        save=False,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        vae_enc = Qmake_encoder_set_weights(input_dim, 128,64,16)\n",
    "        vae_dec = Qmake_decoder_set_weights(input_dim, 128,64,16)\n",
    "        self.model = VAE_Model(vae_enc, vae_dec)\n",
    "        self.model.load_weights(model_path)\n",
    "        self.encoder = self.model.get_layer(\"encoder\")\n",
    "        self.signal = signal\n",
    "        self.background = background\n",
    "        self.br_loss = []\n",
    "        self.signal_loss = []\n",
    "        self.background_outputs = []\n",
    "        self.signal_outputs = []\n",
    "        self.title = title\n",
    "        self.saveplots = save\n",
    "        self.labels = labels\n",
    "        self.latent_info = []\n",
    "        self.br_weights = br_weights\n",
    "        self.signal_weights = signal_weights\n",
    "\n",
    "    def calculate_loss(self, l_type):\n",
    "        self.signal_loss = []\n",
    "        self.br_loss = []\n",
    "        br = self.background\n",
    "\n",
    "        if l_type == \"CKL\":\n",
    "            br_latent = np.array(self.encoder.predict(br))\n",
    "            self.latent_info += [br_latent[0]]\n",
    "            l = []\n",
    "            for i in range(0, br.shape[0]):\n",
    "                loss = AD_score_CKL(br_latent[0][i], br_latent[1][i])\n",
    "                l += [loss]\n",
    "            self.br_loss = l\n",
    "\n",
    "            for i, batch in enumerate(self.signal):\n",
    "                sg_latent = np.array(self.encoder.predict(batch))\n",
    "                self.latent_info += [sg_latent[0]]\n",
    "                l = []\n",
    "\n",
    "                for i in range(0, batch.shape[0]):\n",
    "                    loss = AD_score_CKL(sg_latent[0][i], sg_latent[1][i])\n",
    "                    l += [loss]\n",
    "\n",
    "                sg_loss = l\n",
    "\n",
    "                self.signal_loss += [sg_loss]\n",
    "\n",
    "        if l_type == \"KL\":\n",
    "            br_latent = np.array(self.encoder.predict(br))\n",
    "            l = []\n",
    "            for i in range(0, br.shape[0]):\n",
    "                loss = AD_score_KL(br_latent[0][i], br_latent[1][i])\n",
    "                l += [loss]\n",
    "            self.br_loss = l\n",
    "\n",
    "            for i, batch in enumerate(self.signal):\n",
    "                sg_latent = np.array(self.encoder.predict(batch))\n",
    "\n",
    "                l = []\n",
    "\n",
    "                for i in range(0, batch.shape[0]):\n",
    "                    loss = AD_score_KL(sg_latent[0][i], sg_latent[1][i])\n",
    "                    l += [loss]\n",
    "\n",
    "                sg_loss = l\n",
    "\n",
    "                self.signal_loss += [sg_loss]        \n",
    "\n",
    "    def ROC(self):\n",
    "        target_fpr = 1e-5\n",
    "        tpr_at_target = []\n",
    "        thresholds_at_target = []\n",
    "\n",
    "        plt.plot(\n",
    "            np.linspace(0, 1, 1000), np.linspace(0, 1, 1000), \"--\", label=\"diagonal\"\n",
    "        )\n",
    "        for j, batch in enumerate(self.signal_loss):\n",
    "            sig_w = self.signal_weights[j]\n",
    "            br_w = self.br_weights\n",
    "            weights = np.concatenate((br_w, sig_w))\n",
    "            truth = []\n",
    "            for i in range(len(self.br_loss)):\n",
    "                truth += [0]\n",
    "            for i in range(len(batch)):\n",
    "                truth += [1]\n",
    "            ROC_data = np.concatenate((self.br_loss, batch))\n",
    "            fpr, tpr, thresholds = sk.roc_curve(truth, ROC_data, sample_weight=weights)\n",
    "            # auc=np.trapz(tpr,fpr)\n",
    "            auc = sk.roc_auc_score(truth, ROC_data)\n",
    "            plt.plot(fpr, tpr, label=self.labels[j] + \": \" + str(round(auc, 3)))\n",
    "            \n",
    "            idx = np.argmin(np.abs(fpr - target_fpr))\n",
    "            tpr_at_target.append(tpr[idx])\n",
    "            thresholds_at_target.append(thresholds[idx])\n",
    "\n",
    "\n",
    "        plt.xlabel(\"fpr\")\n",
    "        plt.xlim(1e-7, 1)\n",
    "        plt.ylim(1e-7, 1)\n",
    "        plt.semilogx()\n",
    "        plt.ylabel(\"tpr\")\n",
    "        plt.semilogy()\n",
    "        plt.title(\"{}_ROC\".format(self.title))\n",
    "        plt.vlines(10**-5, 0, 1, colors=\"r\", linestyles=\"dashed\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        if self.saveplots == True:\n",
    "            plt.savefig(\n",
    "                \"/fs/ddn/sdf/group/atlas/d/hjia625/ToyVAE/software_dev/plots/large/{}_ROC.png\".format(\n",
    "                    self.title\n",
    "                ),\n",
    "                format=\"png\",\n",
    "                bbox_inches=\"tight\",\n",
    "            )\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nTPR at FPR = {target_fpr} for each channel:\")\n",
    "        for label, tpr, threshold in zip(self.labels, tpr_at_target, thresholds_at_target):\n",
    "            print(f\"{label}: {tpr*100:.6f}%, Theshold = {threshold:.6f}\")\n",
    "\n",
    "    def GetPerformance(self):\n",
    "        target_fpr = 1e-5\n",
    "        tpr_at_target = []\n",
    "\n",
    "        print(f\"Number of signal losses: {len(self.signal_loss)}\")\n",
    "        print(f\"Number of labels: {len(self.labels)}\")\n",
    "        print(f\"Number of signal weights: {len(self.signal_weights)}\")\n",
    "        print(f\"Length of br_loss: {len(self.br_loss)}\")\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(np.linspace(0, 1, 1000), np.linspace(0, 1, 1000), \"--\", label=\"diagonal\")\n",
    "\n",
    "        for j, batch in enumerate(self.signal_loss):\n",
    "            print(f\"Processing batch {j}\")\n",
    "            print(f\"Batch length: {len(batch)}\")\n",
    "            print(f\"Signal weight length: {len(self.signal_weights[j])}\")\n",
    "\n",
    "            sig_w = self.signal_weights[j]\n",
    "            br_w = self.br_weights\n",
    "            weights = np.concatenate((br_w, sig_w))\n",
    "            truth = np.concatenate([np.zeros(len(self.br_loss)), np.ones(len(batch))])\n",
    "            ROC_data = np.concatenate((self.br_loss, batch))\n",
    "\n",
    "            print(f\"ROC_data shape: {ROC_data.shape}\")\n",
    "            print(f\"truth shape: {truth.shape}\")\n",
    "            print(f\"weights shape: {weights.shape}\")\n",
    "\n",
    "            try:\n",
    "                fpr, tpr, _ = sk.roc_curve(truth, ROC_data, sample_weight=weights)\n",
    "                auc = sk.roc_auc_score(truth, ROC_data)\n",
    "\n",
    "                plt.plot(fpr, tpr, label=f\"{self.labels[j]}: {auc:.3f}\")\n",
    "\n",
    "                idx = np.argmin(np.abs(fpr - target_fpr))\n",
    "                tpr_at_target.append(tpr[idx])\n",
    "\n",
    "                print(f\"Successfully processed batch {j}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {j}: {str(e)}\")\n",
    "\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.title(f\"{self.title} ROC\")\n",
    "        plt.vlines(target_fpr, 0, 1, colors=\"r\", linestyles=\"dashed\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        if self.saveplots:\n",
    "            plt.savefig(\n",
    "                f\"/fs/ddn/sdf/group/atlas/d/hjia625/ToyVAE/software_dev/plots/large/{self.title}_ROC.png\",\n",
    "                format=\"png\",\n",
    "                bbox_inches=\"tight\",\n",
    "            )\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nTPR at FPR = {target_fpr} for each channel:\")\n",
    "        results = list(zip(self.labels, tpr_at_target))\n",
    "        for label, tpr in results:\n",
    "            print(f\"{label}: {tpr*100:.6f}%\")\n",
    "\n",
    "        print(f\"Number of results: {len(results)}\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_h5_data(file_path):\n",
    "#     try:\n",
    "#         with h5py.File(file_path, 'r') as f:\n",
    "#             data = scaler.transform(f['Data'])\n",
    "# #             data = scale_columns(np.array(f['Data']))\n",
    "#         return data\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "#         return None\n",
    "# Ato4l_data = read_h5_data(signals_file[0])\n",
    "# hToTauTau_data = read_h5_data(signals_file[1])\n",
    "# hChToTauNu_data = read_h5_data(signals_file[2])\n",
    "# leptoquark_data = read_h5_data(signals_file[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9afb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses=['CKL']\n",
    "for string in Losses:\n",
    "    evaluation=Model_Evaluator('/fs/ddn/sdf/group/atlas/d/hjia625/ToyVAE/software_dev/software_model/checkpoint_large/',\n",
    "                               X_test,\n",
    "                               np.ones(len(X_test)),\n",
    "                               signal_data,\n",
    "                               [np.ones(len(Ato4l_data)),\n",
    "                                    np.ones(len(hToTauTau_data)),\n",
    "                                    np.ones(len(hChToTauNu_data)),\n",
    "                                    np.ones(len(leptoquark_data))],\n",
    "                               input_dim = X_test.shape[1],\n",
    "                               title='VAE Model',\n",
    "                               save = False,\n",
    "                               labels = signal_labels)\n",
    "    evaluation.calculate_loss(string)\n",
    "    evaluation.ROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses=['KL']\n",
    "for string in Losses:\n",
    "    evaluation=Model_Evaluator('/eos/user/h/hjia/AnomalyDetection/trained_models/VAE_model/version1/',\n",
    "                               X_test,\n",
    "                               np.ones(len(X_test)),\n",
    "                               signal_data,\n",
    "                               [np.ones(len(Ato4l_data)),\n",
    "                                    np.ones(len(hToTauTau_data)),\n",
    "                                    np.ones(len(hChToTauNu_data)),\n",
    "                                    np.ones(len(leptoquark_data))],\n",
    "                               input_dim = X_test.shape[1],\n",
    "                               title='VAE Model',\n",
    "                               save = False,\n",
    "                               labels = signal_labels)\n",
    "    evaluation.calculate_loss(string)\n",
    "    evaluation.ROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "signal_names = [\"Ato4l\",\n",
    "                \"hToTauTau\",\n",
    "                \"hChToTauNu\",\n",
    "                \"leptoquark\"]\n",
    "channel_results = {}\n",
    "for i in range(20):\n",
    "    try:\n",
    "        model_path = f'//fs/ddn/sdf/group/atlas/d/hjia625/ToyVAE/software_dev/software_model/checkpoint_large/Repeat_Testing/large_{i}/'\n",
    "        print(f\"Evaluating model from path: {model_path}\")\n",
    "        evaluation = Model_Evaluator(model_path,\n",
    "                               X_test,\n",
    "                               np.ones(len(X_test)),\n",
    "                               signal_data,\n",
    "                               [np.ones(len(Ato4l_data)),\n",
    "                                    np.ones(len(hToTauTau_data)),\n",
    "                                    np.ones(len(hChToTauNu_data)),\n",
    "                                    np.ones(len(leptoquark_data))],\n",
    "                               input_dim = X_test.shape[1],\n",
    "                               title='VAE Model {i}',\n",
    "                               save = False,\n",
    "                               labels = signal_labels)\n",
    "        \n",
    "        print(\"Model_Evaluator instance created successfully\")\n",
    "        \n",
    "        print(\"Calculating losses...\")\n",
    "        evaluation.calculate_loss('KL')\n",
    "        print(\"Losses calculated\")\n",
    "        \n",
    "        print(\"Getting performance...\")\n",
    "        result = evaluation.GetPerformance()\n",
    "        print(f\"GetPerformance() returned: {result}\")\n",
    "        \n",
    "        if not result:\n",
    "            print(f\"Warning: GetPerformance() returned an empty result for model {i}\")\n",
    "        else:\n",
    "            for channel, tpr in result:\n",
    "                if channel not in channel_results:\n",
    "                    channel_results[channel] = []\n",
    "                channel_results[channel].append(tpr)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating model {i}:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "print(\"Final channel results:\", channel_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if channel_results:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    channels = list(channel_results.keys())\n",
    "    n_channels = len(channels)\n",
    "    n_models = len(channel_results[channels[0]])\n",
    "    \n",
    "    x = np.arange(n_models)\n",
    "    width = 0.8 / n_channels\n",
    "    \n",
    "    for i, channel in enumerate(channels):\n",
    "        tprs = channel_results[channel]\n",
    "        plt.bar(x + i * width, tprs, width, label=channel)\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('True Positive Rate (TPR) at FPR = 1e-5')\n",
    "    plt.title('TPR Comparison Across Models and Channels')\n",
    "    plt.xticks(x + width * (n_channels - 1) / 2, [f'Model {i}' for i in range(n_models)])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('VAE_tpr_comparison_plot.png', dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nChannel Statistics:\")\n",
    "    for channel in channels:\n",
    "        tprs = channel_results[channel]\n",
    "        mean = np.mean(tprs)\n",
    "        std = np.std(tprs)\n",
    "        print(f\"{channel}: Mean TPR = {mean*100:.4f}%, Std Dev = {std:.4f}\")\n",
    "else:\n",
    "    print(\"No results were obtained. Cannot create comparison plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe94984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if channel_results:\n",
    "    plt.figure(figsize=(14, 8))  # Increased figure size to accommodate the legend\n",
    "    channels = list(channel_results.keys())\n",
    "    n_channels = len(channels)\n",
    "    n_models = len(channel_results[channels[0]])\n",
    "    \n",
    "    for i, channel in enumerate(channels):\n",
    "        tprs = channel_results[channel]\n",
    "        plt.scatter(range(n_models), tprs, label=channel, s=50)\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('True Positive Rate (TPR) at FPR = 1e-5')\n",
    "    plt.semilogy()\n",
    "    plt.title('TPR Comparison Across Models and Channels')\n",
    "    plt.xticks(range(n_models), [f'{i}' for i in range(n_models)])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Place legend outside the plot\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('VAE_tpr_comparison_plot.png', dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nChannel Statistics:\")\n",
    "    for channel in channels:\n",
    "        tprs = channel_results[channel]\n",
    "        mean = np.mean(tprs)\n",
    "        std = np.std(tprs)\n",
    "        print(f\"{channel}: Mean TPR = {mean:.4f}, Std Dev = {std:.4f}\")\n",
    "else:\n",
    "    print(\"No results were obtained. Cannot create comparison plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_vae_enc = Qmake_encoder_set_weights(57,128,64,16)\n",
    "ori_vae_dec = Qmake_decoder_set_weights(57,128,64,16)\n",
    "orginal_model = VAE_Model(ori_vae_enc, ori_vae_dec)\n",
    "# orginal_model.predict(Ato4l_data)\n",
    "orginal_model.load_weights(\"/eos/user/h/hjia/AnomalyDetection/trained_models/VAE_model/version1/\")\n",
    "\n",
    "\n",
    "def make_simplified_encoder(input_dim, h_dim_1, h_dim_2, latent_dim):\n",
    "    inputs = keras.Input(shape=(input_dim,), name='inputs')\n",
    "#     x = BatchNormalization(name=\"BN0\")(inputs)\n",
    "    x = Dense(h_dim_1,\n",
    "             kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "             bias_initializer=keras.initializers.Zeros(),\n",
    "             name = \"dense1\")(inputs)\n",
    "#     x = BatchNormalization(name=\"BN1\")(x)\n",
    "    x = ReLU(name=\"relu1\")(x)\n",
    "    x = Dense(h_dim_2,\n",
    "             kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "             bias_initializer=keras.initializers.Zeros(),\n",
    "             name = \"dense2\")(x)    \n",
    "#     x = BatchNormalization(name=\"BN2\")(x)\n",
    "    x = ReLU(name=\"relu2\")(x)\n",
    "    z_mean=Dense(latent_dim, name='z_mean',\n",
    "                  kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                  bias_initializer=keras.initializers.Zeros())(x)\n",
    "    new_encoder = keras.Model(inputs,z_mean,name='encoder')\n",
    "    return new_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_encoder = make_simplified_encoder(57,128,64,16)\n",
    "# Transfer weights from the original encoder\n",
    "for layer in new_encoder.layers:\n",
    "    original_layer = ori_vae_enc.get_layer(layer.name)\n",
    "    if original_layer is not None:\n",
    "        print(\"set weight for \", layer)\n",
    "        print(original_layer.get_weights())\n",
    "        layer.set_weights(original_layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(new_encoder, 'VAE_40MHZ_model_Vlarge_onchip', save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
